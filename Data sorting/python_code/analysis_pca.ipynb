{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddb5683-543f-46b4-b163-b8722c6b67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ddd89d-d838-4464-95c0-7c6b0ba0f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "class analysis_pca:\n",
    "    \"\"\"\n",
    "    need filter_matrix.py\n",
    "    Ts: sampling rate\n",
    "    data: spike data from pre_processing.datasorting_orientation, \"cells X directions X trials X time\" array\n",
    "    save_path: save path\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, Ts, data, save_path):\n",
    "        self.Ts = Ts\n",
    "        self.data = data\n",
    "        self.data_size = np.shape(data)\n",
    "        self.save_path = save_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e679b0b4-fcb5-4be6-be39-6645eafd8648",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T06:09:32.789588Z",
     "iopub.status.busy": "2025-04-29T06:09:32.789215Z",
     "iopub.status.idle": "2025-04-29T06:09:32.793542Z",
     "shell.execute_reply": "2025-04-29T06:09:32.793055Z",
     "shell.execute_reply.started": "2025-04-29T06:09:32.789571Z"
    }
   },
   "outputs": [],
   "source": [
    "def smoothing_data(self, sigma = 2):\n",
    "    \"\"\"\n",
    "    Generate spike data with averaged and smoothing by trials.\n",
    "\n",
    "    Parameters:\n",
    "        sigma: float, optional\n",
    "            Standard deviation of the Gaussian kernel (default: 2). See details at \"filter_matrix.py\"\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray (directions X cells X time)\n",
    "            Filtered and averaged spike data.\n",
    "    \"\"\"\n",
    "\n",
    "    ds = self.data_size[1] - 1  # number of direction, subtract 1 because last column would be rejected trials\n",
    "    data_cat = self.data\n",
    "\n",
    "    # preprocessing\n",
    "    data_total = []\n",
    "    for a in range(ds): # orientation\n",
    "        temp = np.array(data_cat).T[a]\n",
    "        t_temp = []\n",
    "        for data_num in range(np.shape(data_cat)[0]):\n",
    "            temp2 = np.mean(temp[0],axis=0)\n",
    "            t_temp.append(temp2)\n",
    "            \n",
    "        data_total.append(t_temp)\n",
    "    \n",
    "    # smothing\n",
    "    from filter_matrix import filter_matrix\n",
    "    \n",
    "    data_cat_sm = []\n",
    "    for angle in range(ds):\n",
    "        temp_cat = [];\n",
    "        for n in range(np.shape(data_cat)[0]):\n",
    "            z = data_cat[n][angle]\n",
    "            zf = filter_matrix(z,2)\n",
    "            temp_cat.append(np.mean(zf,axis=0))\n",
    "            \n",
    "        data_cat_sm.append(temp_cat)\n",
    "    \n",
    "    data_cat_sm = np.array(data_cat_sm)\n",
    "\n",
    "    return data_cat_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f960c65f-29e7-4e04-85e7-e52c20ddea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_analy(self, data):\n",
    "    \"\"\"\n",
    "    Generate PCA data from averaged spike data.\n",
    "\n",
    "    Parameters:\n",
    "        sigma: float, optional\n",
    "            Standard deviation of the Gaussian kernel (default: 2). See details at \"filter_matrix.py\"\n",
    "    \n",
    "    Returns:\n",
    "        dictionary[direction]\n",
    "        {'v': pca components, 'p': pca transform data, 'dd': explained variance_ratio}\n",
    "    \"\"\"\n",
    "\n",
    "    data_cat_sm = data\n",
    "    ds = self.data_size[1] - 1  # number of direction, subtract 1 because last column would be rejected trials\n",
    "    \n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    PCA_total = {}\n",
    "    \n",
    "    # Loop over the orientations (assuming 8 orientations)\n",
    "    for ori in range(ds):\n",
    "        temp_data = data_cat_sm[ori].T  \n",
    "        pca = PCA()\n",
    "        pca.fit(temp_data)  # Perform PCA\n",
    "        v = pca.components_   # coeff\n",
    "        p = pca.transform(temp_data).T  # scores\n",
    "        dd = pca.explained_variance_ratio_   # explained variance\n",
    "    \n",
    "        # Get the explained variance ratio and cumulative sum\n",
    "        var_explained = np.cumsum(dd) * 100\n",
    "        \n",
    "        # Print the variance explained for the first 5 components\n",
    "        for i in range(5):\n",
    "            print(f'Dimensions: {i+1}, Variance explained: {var_explained[i]:.2f}%')\n",
    "    \n",
    "        # Find the dimension to reduce to based on cumulative variance explained\n",
    "        nmode = np.argmax(var_explained > 87.5) + 1\n",
    "        print(f'Dimensions to be reduced: {nmode}')\n",
    "        \n",
    "        # Store PCA results for each orientation\n",
    "        PCA_total[ori] = {'v': pca.components_, 'p': pca.transform(temp_data), 'dd': pca.explained_variance_ratio_}\n",
    "\n",
    "    return PCA_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66d7e3e-0317-4b3d-bf63-1a0f3736c2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_to_pca(self, data, dim):\n",
    "    \"\"\"\n",
    "    Generate PCA data from PCA data.\n",
    "\n",
    "    Parameters:\n",
    "        dictionary[direction]\n",
    "        {'v': pca components, 'p': pca transform data, 'dd': explained variance_ratio}\n",
    "        dim: number of dimension that will be used for this analysis\n",
    "    \n",
    "    Returns:\n",
    "        dictionary\n",
    "        {'v': pca components, 'p': pca transform data, 'dd': explained variance_ratio}\n",
    "    \"\"\"\n",
    "\n",
    "    PCA_total = data\n",
    "    ds = self.data_size[1] - 1  # number of direction, subtract 1 because last column would be rejected trials\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    pca_data = []\n",
    "    for a in range(ds):\n",
    "        temp = PCA_total[a]['p']\n",
    "        temp = temp[:,:dim]\n",
    "        pca_data.append(temp)\n",
    "    \n",
    "    pca_data = np.array(pca_data)\n",
    "    pca_data = np.concatenate(pca_data, axis = 1)\n",
    "\n",
    "    # PCA_to_PCA analysis\n",
    "    PCA_total2 = {}\n",
    "\n",
    "    pca = PCA()\n",
    "    pca.fit(pca_data)  # Perform PCA\n",
    "    v = pca.components_   # coeff\n",
    "    p = pca.transform(pca_data).T  # scores\n",
    "    dd = pca.explained_variance_ratio_   # explained variance\n",
    "    \n",
    "    # Get the explained variance ratio and cumulative sum\n",
    "    var_explained = np.cumsum(dd) * 100\n",
    "    \n",
    "    # Print the variance explained for the first 5 components\n",
    "    for i in range(5):\n",
    "        print(f'Dimensions: {i+1}, Variance explained: {var_explained[i]:.2f}%')\n",
    "    \n",
    "    # Find the dimension to reduce to based on cumulative variance explained\n",
    "    nmode = np.argmax(var_explained > 95) + 1  # Adding 1 since indexing is 1-based in MATLAB\n",
    "    print(f'Dimensions to be reduced: {nmode}')\n",
    "    \n",
    "    # Store PCA results for each orientation\n",
    "    PCA_total2 = {'v': pca.components_, 'p': pca.transform(temp_data), 'dd': pca.explained_variance_ratio_}\n",
    "\n",
    "    return PCA_total2\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad4269-3e5e-45ea-9327-b97a72d61c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot variance explained\n",
    "plt.figure()\n",
    "plt.plot(var_explained[:20], 'o-', label=\"Cumulative variance explained\")\n",
    "plt.xlabel('Dimensions')\n",
    "plt.ylabel('Variance explained (%)')\n",
    "plt.title('Variance Explained')\n",
    "\n",
    "# plt.savefig('var_ex.jpg', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot the principal component weights (the eigenvectors)\n",
    "plt.figure()\n",
    "plt.plot(v.T)  # The components are in rows, so transpose for plotting\n",
    "plt.xlabel('Cells')\n",
    "plt.ylabel('Weights')\n",
    "plt.title('Principal Component Weights')\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('pc_weight.jpg', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot the principal component time series (scores)\n",
    "plt.figure()\n",
    "# hx = []\n",
    "for i in range(min(nmode, 4)):  # Plot the first 4 components or up to nmode\n",
    "    ax = plt.subplot(4, 1, i + 1)\n",
    "    # hx.append(ax)\n",
    "    ax.plot(np.arange(-300, 300), p[i, :600])  # Assuming 600 time steps\n",
    "    ax.set_title(f'dPC {i + 1}')\n",
    "    ax.axis('tight')\n",
    "    ax.get_xaxis().set_ticks([])\n",
    "plt.xlabel('Time after saccade onset (ms)')\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('pc.jpg', dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
